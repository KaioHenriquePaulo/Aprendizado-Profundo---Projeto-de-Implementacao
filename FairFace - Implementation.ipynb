{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acea0ac",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb94615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Plots e avaliação\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import umap.umap_ as umap\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Ignora todos os avisos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f40ce",
   "metadata": {},
   "source": [
    "## Execute abaixo, apenas se não tiver o _dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b153fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download da versão mais recente\n",
    "\n",
    "# path = kagglehub.dataset_download(\"aibloy/fairface\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60beb35",
   "metadata": {},
   "source": [
    "## Dicionário de Configuração\n",
    "\n",
    "Contém todos os itens necessários para a execução do _pipeline_ completo. Desde tamanho de _batch_ e taxa de aprendizado até o tipo de modelo usado e localização do _dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando resnet101 com ArcFace=True em: cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'batch_size': 128,          \n",
    "    'lr': 0.0001,                \n",
    "    'epochs': 10,\n",
    "    'num_classes': 7,            # Ex: FairFace (White, Black, Indian, East/SE Asian, Middle East, Hispanic)\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'data_dir': 'FairFace',     \n",
    "    'model_name': 'vit_b_16',    # Opções: 'vit_b_16', 'resnet50', 'resnet34', 'resnet101', 'efficientnet', 'vgg16'\n",
    "    'use_arcface': True,         # LIGA/DESLIGA o ArcFace\n",
    "    'embedding_size': 512       \n",
    "}\n",
    "\n",
    "print(f'Executando {CONFIG[\"model_name\"]} com ArcFace={CONFIG[\"use_arcface\"]} em: {CONFIG[\"device\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8b39d",
   "metadata": {},
   "source": [
    "## Classe ArcFace\n",
    "\n",
    "Usado como parte do modelo para o reconhecimento de faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336d0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label=None):\n",
    "        # 1. Normaliza features e pesos \n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        \n",
    "        if label is None:\n",
    "            return cosine * self.s\n",
    "\n",
    "        # 2. Aplica a margem angular apenas na classe correta\n",
    "        # cos(theta + m) = cos(theta)cos(m) - sin(theta)sin(m)\n",
    "        phi = cosine - self.m # Aproximação simplificada robusta para treinamento\n",
    "        \n",
    "        # One-hot encoding para aplicar a margem apenas no target\n",
    "        one_hot = torch.zeros(cosine.size(), device=CONFIG['device'])\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        # Logits finais: classe correta ganha penalidade, forçando aprendizado\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cf8d7",
   "metadata": {},
   "source": [
    "## Classe ModelWrapper\n",
    "\n",
    "Utilizada para extrair _embeddings_ a partir dos modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc012c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, base_model, input_features, num_classes, use_arcface=False):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.features = base_model\n",
    "        self.use_arcface = use_arcface\n",
    "        \n",
    "        # Camada de projeção para garantir tamanho fixo do embedding\n",
    "        self.embedding_layer = nn.Linear(input_features, CONFIG['embedding_size'])\n",
    "        self.bn = nn.BatchNorm1d(CONFIG['embedding_size']) # BN ajuda muito no ArcFace\n",
    "        \n",
    "        if use_arcface:\n",
    "            self.classifier = ArcFaceLayer(CONFIG['embedding_size'], num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(CONFIG['embedding_size'], num_classes)\n",
    "        \n",
    "    def forward(self, x, labels=None):\n",
    "        # Extração de features\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        \n",
    "        # Gera Embedding\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        embeddings = self.bn(embeddings)\n",
    "        \n",
    "        # Classificação (ArcFace precisa dos labels no treino)\n",
    "        if self.use_arcface and labels is not None:\n",
    "            logits = self.classifier(embeddings, labels)\n",
    "        else:\n",
    "            logits = self.classifier(embeddings)\n",
    "            \n",
    "        return logits, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af18e0",
   "metadata": {},
   "source": [
    "## Instanciamento do Modelo\n",
    "\n",
    "Dependendo da configuração selecionada na célula de CONFIG, é selecionada um modelo para treinamento. O modelo será baixado com os pesos padrões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7036512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes):\n",
    "    print(f\"Carregando {model_name}...\")\n",
    "    \n",
    "    if model_name == 'vit_b_16':\n",
    "        # Vision Transformer SOTA\n",
    "        base = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "        num_ftrs = base.heads.head.in_features\n",
    "        base.heads = nn.Identity() # Remove a cabeça original\n",
    "\n",
    "    elif model_name == 'resnet34':\n",
    "        base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        num_ftrs = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        \n",
    "    elif model_name == 'resnet50':\n",
    "        base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "    \n",
    "    elif model_name == 'resnet101':\n",
    "        base = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        num_ftrs = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        \n",
    "    elif model_name == 'efficientnet':\n",
    "        base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        num_ftrs = base.classifier[1].in_features\n",
    "        base.classifier = nn.Identity()\n",
    "        \n",
    "    elif model_name == 'vgg16':\n",
    "        base = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        num_ftrs = base.classifier[6].in_features\n",
    "        base.classifier[6] = nn.Identity()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Modelo desconhecido\")\n",
    "\n",
    "    model = ModelWrapper(base, num_ftrs, num_classes, CONFIG['use_arcface'])\n",
    "    return model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34b753",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "Nesta célua, é definida a função de treinamento do modelo, obtendo métricas interessantes ao longo das épocas e realizando todo o processo de ajuste de pesos. Ao final, retorna o modelo com os pesos treinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a38ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        print(f'Epoch {epoch+1}/{CONFIG[\"epochs\"]}')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(CONFIG['device'])\n",
    "                labels = labels.to(CONFIG['device'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # ArcFace precisa dos labels no forward pass durante o treino\n",
    "                    if phase == 'train' and CONFIG['use_arcface']:\n",
    "                        logits, _ = model(inputs, labels)\n",
    "                    else:\n",
    "                        logits, _ = model(inputs) # Validação ou Softmax normal\n",
    "                        \n",
    "                    loss = criterion(logits, labels)\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfb269",
   "metadata": {},
   "source": [
    "## Gráficos de evolução\n",
    "\n",
    "A partir das métricas de histórico obtidas no treinamento do modelo, esta célula plota os gráficos de evolução da _loss_ e acurácia do conjunto de treino e validação ao longo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69876e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Subplot 1 - Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Treino Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validação Loss')\n",
    "    plt.title('Evolução da Loss')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Subplot 2 - Acurácia\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['val_acc'], 'g-', label='Validação Acc')\n",
    "    plt.title('Evolução da Acurácia de Validação')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93348e9",
   "metadata": {},
   "source": [
    "## Criação do _dataset_\n",
    "\n",
    "O dataset FairFace é composto por duas pastas de imagens chamadas \"_train_\" e \"_val_\" e dois arquivos CSV dedicados a cada uma das pastas contendo informações sobre gênero, idade e raça das imagens presentes na base de dados. Nesta célula, extraímos apenas a informação relevante para o projeto, a raça. E deixamos a separação de treinamento e validação inalteradas em relação à forma que vieram no _dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8bf34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, class_map=None):\n",
    "\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Cria mapeamento automático de classes (String -> Int) se não for fornecido\n",
    "        if class_map is None:\n",
    "            self.classes = sorted(self.annotations['race'].unique())\n",
    "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_map\n",
    "            self.classes = list(class_map.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Obtém o nome da imagem do CSV.\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx]['file'])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except (IOError, FileNotFoundError):\n",
    "            # Fallback de segurança para não quebrar o treino por 1 imagem corrompida\n",
    "            print(f'Aviso: Imagem não encontrada ou corrompida: {img_name}')\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        # Obtém o label e converte para índice\n",
    "        race_label_str = self.annotations.iloc[idx]['race']\n",
    "        label = self.class_to_idx[race_label_str]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_dataloaders():\n",
    "    # Define os caminhos exatos\n",
    "    BASE_DIR = './FairFace' # Ajuste conforme o ambiente\n",
    "    TRAIN_CSV = os.path.join(BASE_DIR, 'train_labels.csv')\n",
    "    VAL_CSV = os.path.join(BASE_DIR, 'val_labels.csv')     \n",
    "    \n",
    "    transforms_list = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    print('Carregando datasets via CSV...')\n",
    "    \n",
    "    # 1. Cria Dataset de Treino\n",
    "    # root_dir aponta para BASE_DIR pois o CSV do FairFace geralmente inclui o prefixo \"train/\" no nome do arquivo\n",
    "    train_dataset = FairFaceDataset(csv_file=TRAIN_CSV, \n",
    "                                    root_dir=BASE_DIR, \n",
    "                                    transform=transforms_list['train'])\n",
    "    \n",
    "    # 2. Cria Dataset de Validação\n",
    "    # Passa o class_map do treino para garantir que \"Asian\" seja o ID 0 em ambos\n",
    "    val_dataset = FairFaceDataset(csv_file=VAL_CSV, \n",
    "                                  root_dir=BASE_DIR, \n",
    "                                  transform=transforms_list['val'],\n",
    "                                  class_map=train_dataset.class_to_idx)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0),\n",
    "        'val': DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "    }\n",
    "    \n",
    "    class_names = train_dataset.classes\n",
    "    print(f'Classes encontradas: {class_names}')\n",
    "    print(f'Tamanho Treino: {len(train_dataset)} | Tamanho Val: {len(val_dataset)}')\n",
    "    \n",
    "    return dataloaders, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13709",
   "metadata": {},
   "source": [
    "## Avaliação\n",
    "\n",
    "Esta célula é responsável por mostrar informações como acurácia, precisão e recall de classes específicas ou do conjunto como um todo. O UMAP também é gerado nesta célula para avaliação do balanceamento da base de dados prevista pelo modelo e a real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b4f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_deia(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    print('\\nExtraindo Embeddings para Análise de Viés...')\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            labels = labels.to(CONFIG['device'])\n",
    "            logits, embeddings = model(inputs)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_embeddings.extend(embeddings.cpu().numpy())\n",
    "\n",
    "    # Métricas\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    # Visualização\n",
    "    X = np.array(all_embeddings)\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    \n",
    "    print('Gerando UMAP (pode demorar um pouco)...')\n",
    "    reducer = umap.UMAP(n_neighbors=20, min_dist=0.1, metric='cosine')\n",
    "    embedding_2d = reducer.fit_transform(X)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Realidade (Labels Verdadeiros)\n",
    "    scatter1 = ax[0].scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=y_true, cmap='tab10', s=15, alpha=0.7)\n",
    "    ax[0].set_title('Espaço Latente: Classes Reais (Ground Truth)')\n",
    "    ax[0].legend(*scatter1.legend_elements(), title='Etnias')\n",
    "    \n",
    "    # Plot 2: Percepção do Modelo (Predições)\n",
    "    scatter2 = ax[1].scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=y_pred, cmap='tab10', s=15, alpha=0.7)\n",
    "    ax[1].set_title('Espaço Latente: Predição do Modelo')\n",
    "    \n",
    "    # Destaque de Erros DEIA\n",
    "    # Pontos onde Pred != Real podem ser circulados ou marcados\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aee8b8",
   "metadata": {},
   "source": [
    "## _Pipeline_ de execução\n",
    "\n",
    "Após a compartimentalização do código, a classe _main_ é responsável por executar todos os processos. Desde o carregamento do modelo, criação do _dataset_ e obtenção das métricas de avaliação do modelo. Altere o dicionário CONFIG para procurar resultados diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando datasets via CSV...\n",
      "Classes encontradas: ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n",
      "Tamanho Treino: 86744 | Tamanho Val: 10954\n",
      "Carregando vgg16...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m], weight_decay=\u001b[32m1e-4\u001b[39m) \u001b[38;5;66;03m# AdamW é melhor para ViT\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Treinamento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model, history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Salva o modelo treinado\u001b[39;00m\n\u001b[32m     16\u001b[39m torch.save(model.state_dict(), \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCheckpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloaders, criterion, optimizer)\u001b[39m\n\u001b[32m     33\u001b[39m             loss.backward()\n\u001b[32m     34\u001b[39m             optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m     running_corrects += torch.sum(preds == labels.data)\n\u001b[32m     39\u001b[39m epoch_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(dataloaders[phase].dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Gera os conjuntos de treino e validação\n",
    "    dataloaders, class_names = get_dataloaders()\n",
    "    CONFIG['num_classes'] = len(class_names)\n",
    "    \n",
    "    # Carrega o modelo e define a função de perda e otimizador\n",
    "    model = get_model(CONFIG['model_name'], CONFIG['num_classes'])\n",
    "    # model.load_state_dict(torch.load('Checkpoints/vit_b_16_2.pth', map_location=CONFIG['device']))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4) # AdamW é melhor para ViT\n",
    "    \n",
    "    # Treinamento\n",
    "    model, history = train_model(model, dataloaders, criterion, optimizer)\n",
    "\n",
    "    # Salva o modelo treinado\n",
    "    torch.save(model.state_dict(), f'Checkpoints/{CONFIG[\"model_name\"]}.pth')\n",
    "\n",
    "    # Plot dos resultados\n",
    "    plot_training_history(history)\n",
    "    # Plot do UMAP do treino\n",
    "    evaluate_deia(model, dataloaders['train'], class_names)\n",
    "    # Plot do UMAP da validação\n",
    "    evaluate_deia(model, dataloaders['val'], class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
